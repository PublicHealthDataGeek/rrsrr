[
["index.html", "Reproducible road safety research with R: A practical introduction Preface", " Reproducible road safety research with R: A practical introduction Robin Lovelace, Malcolm Morgan, Andrea Gilardi Preface Many areas of research have real world implications, but few have the ability to save lives in the way that road safety research does. Road safety research is a data driven field of research, underpinned by attribute-rich spatio-temporal event-based datasets representing the grim reality of people who are tragically hurt or killed on the roads. Because of the incessant nature of road casualties there is a danger that it becomes normalised, an implicitly accepted cost associated with the benefits of personal mobility. Data analysis in general and ‘data science’ in particular has great potential to support more evidence-based road safety policies. Data science can be defined as a particular type of data analysis process that is script-based, reproducible and scalable. As such, it has the ability to represent what we know about road casualties in new ways, demonstrate the life-saving impacts of effective policies and prioritise interventions that are most likely to work. This book is designed not to be a static textbook that is read once and accumulates dust. It is meant to be a hand-book, taken out into the field of applied research and referred to frequently in the course of an analysis project. As such, it is applied and exercise based. There are strong links between data science, open data, open source software and more collaborative ways of working. As such, this book is itself an open source, collaborative and open source project that is designed to be a living document. We encourage any comments, questions or contributions related to its contents, the source code of which can be found at the Reproducible Road Safety Research with R (rrsrr) repo on the saferactive GitHub organisation, via the issue tracker. More broadly, we hope you enjoy the contents of the book and find the process of converting data science into data driven policy changes and investment rewarding. Get ready for and enjoy the ride! "],
["introduction.html", "1 Introduction 1.1 Reproducibility 1.2 What is R? 1.3 Why R for road safety research? 1.4 Prerequisites 1.5 Installing R and RStudio 1.6 R in the cloud 1.7 Recommended packages 1.8 Overview", " 1 Introduction This book teaches reproducible road safety analysis with R. It was initially developed for a 2 day Introduction to R for Road Safety course. Since then, interest in the topic has grown. The RAC Foundation charity in the UK funded the development of this book as a free and open resource to support their objective of making the roads safer for everyone. The content is based on open access road crash data from the UK, which is provided by the R package stats19 (Lovelace et al. 2019). However, the content is designed to be general and should be of use to anyone working with road crash data worldwide that has (at a minimum): A timestamp A location (or address that can be geocoded) Attribute data, such as severity of crash, and type of vehicles involved The book is based on open source software, which knows no borders: R works equally well in China, India, the USA and the UK. The book is practical, meaning that you should reproduce the examples that are provided throughout. As with many practical skills you learn data science by doing data science. Before getting stuck in with the practical content, which begins in Chapter 2, the rest of this chapter: introduces the concept reproducibility and its importance for evidence-based policies explains the choice of R as a ‘tool of the trade’ for road safety research outlines how to install R on your computer or access it through remote servers in the ‘cloud’, and explains the structure of the book, outlines chapter contents and how they should be used for maximum benefit depending on your level of experience and aims (Section 1.8) 1.1 Reproducibility Reproducible research can be defined as work that generates results that can be re-generated by others using publicly accessible code (Lovelace, Nowosad, and Muenchow 2019). By contrast, findings that cannot be repeated are not reproducible. Reproducibility is not a binary concept but a continuum. On one hand, there is work that does not report the data source, methods or software. On the other hand are findings that can be reproduced in their entirety, including the production of figures and, as is the case with this book, the manuscript/medium in which results are presented. Reproducibility can be built into every stage of quantitative research, as shown in Table 1.1.1 Table 1.1: Four elements of reproducibility adapted from Peng, Dominici, and Zeger (2006). Research component Requirement Data Datasets used are available. Methods Computer code underlying figures, tables, etc are available. Software to execute that code is available. Documentation Documentation of the computer code, software environment, and methods for others to repeat and build on the analysis. Distribution Standard methods of distribution are used for others to access the software, data, and documentation. The importance of reproducibility in scientific research should be obvious: if findings cannot be repeated, this casts doubt on the validity, truth, of the conclusions drawn from them. Reproducibility is vital for falsifiability, a cornerstone of science (Popper 1959). In applied, policy-relevant research areas such as road safety research, reproducibility is equally important: policy makers and the public want to have confidence that the evidence underlying key decisions is reliable. Policies based on results that nobody can reproduce are based are harder to defend than policies that based on clear evidence base that others, including members of the public and educators, can repeat. In transport planning, open and reproducible methods support more transparent and democratically accountable interventions (Lovelace, Parkin, and Cohen 2020). Reproducibility lead to solid science which is conducive to effective policies. In the context of road safety research, this means that reproducibility can save lives. 1.2 What is R? R is an open source programming language first developed by award-winning academic statisticians Dr Ross Ihaka and Professor Robert Gentleman. Since its first release in 1995 and the release of version 1.0.0 in 2000, R has seen rapid uptake (Gentleman and Temple Lang 2007; Ihaka, Gentleman, and Robert 1996). As of September 2020, R is ranked as the 9th most used programming language on the TIOBE Index, ahead of other languages for data processing such as SQL and MATLAB and behind general purpose languages such as C, Java and Python. An important feature of R is that it was designed for data processing and statistical analysis. That means that you can undertake many aspects of road safety research using the core language.2 R is widely acknowledged to outperform other open languages for data science such as Julia, Python and Scala in terms of data visualisation and deployment of web applications for presenting data via the R package shiny (Wickham 2020).3 Furthermore, recently developed tidyverse and sf packages provide a unified and user friendly system for working with attribute-rich and geographic datasets (Grolemund and Wickham 2016; Lovelace, Nowosad, and Muenchow 2019). Because road crash data is commonly attribute-rich and geographic, we will be using these packages in subsequent chapters. 1.3 Why R for road safety research? R is an outstanding language for reproducible research (Lovelace, Nowosad, and Muenchow 2019; Peng, Dominici, and Zeger 2006). It is accessible, with no licensing restrictions and easy installation procedures on a wide range of computers including on most versions of Windows, Mac and Linux operating systems (R Core Team 2020a). Furthermore, R is highly extensible. With 15,000+ packages available, many of which are developed by professional statisticians and domain experts, R provides access to a wide array of statistical, computational and visualisation techniques. Many packages, such as markdown and reprex, were designed to support more reproducible research (Xie, Allaire, and Grolemund 2018; Bryan et al. 2019). From a road safety perspective, R is well suited to handling data structures used in road safety research. R excels at the processing, analysis, modelling and visualisation of large spatio-temporal and attribute-rich datasets of the type key to road safety research. R is a mature and growing tool for data science popular in industry, academia and government, so creates multiple opportunities for collaboration, within and between organisations and internationally. It is important to have an up-to-date version of R installed before proceeding to the practical sections. This and other prerequisites are covered in the next section. 1.4 Prerequisites You do not need to be a professional programmer, data scientist or computer wizard to use R for road safety research. If you have primarily used graphical user interfaces (GUIs) such as Microsoft Excel for your work, the code-based approach may take some getting used to. But the command-line interface (CLI) of R is no ‘harder’ than the incessant pointing and clicking demanded by tools such as Excel and web-based GUIs for road safety research. It takes time to adapt to new ways of working and R has a steep learning curve at the outset. However, persevering can be very rewarding: proficiency with R’s CLI is a future-proof and transferable skill that can yield huge productivity gains. Perhaps the most important prerequisite, therefore, is time and a willingness to try new ways of working. The good news is that it has never been easier to install and learn R, as highlighted in the stats19-training-setup that can be found on the stats19 package website at docs.ropensci.org/stats19. Note that like any actively developed software, R is evolving, so it is worth updating or re-installing R/RStudio every year or so and updating your R packages every month or so, to ensure you have the latest software. 1.5 Installing R and RStudio If you plan to use R on your own computer or your organisation’s computer (recommended for most people), the installation steps are as follows: Install R from cran.r-project.org Install RStudio from rstudio.com Install R packages, by opening RStudio and typing install.packages(\"stats19\") in the console to install the stats19 package, for example (see Section 1.7 for details) We recommend using at least the latest stable release of R (4.0.0 at the time of writing in 2020). We recommend running R on a decent computer, with at least 4 GB RAM and ideally 8 or more GB and a modern processor. R is computationally efficient and therefore fast language for data science but, because of the size of some road safety datasets, we recommend using it on a high spec laptop or desktop. 1.6 R in the cloud If you do not have access to a suitable computer on which you can install R or just want to get up-and-running quickly, you can run R in the cloud.4 Various organisations manage RStudio Server instances but by far the most well-known cloud provider at https://cloud.rstudio.com/. To run R in the cloud, sign-up to cloud.rstudio.com (or cloud instance of your choice) and acess RStudio from the browser. 1.7 Recommended packages Of the thousands of available packages that are available for road safety research, we will use a handful that are mature, well-tested and well-suited to statistical analysis and visualisation of road casualty data. For the first practical section, in Chapter 2, all you need is a working version of R and RStudio. In Chapter 4 we will see how to install and use add-on packages such as stats19. If you want to be ahead of the game, however, you can check that you have the necessary packages installed by running the following commands, which install and load the packages for we will use for the course: install.packages(&quot;remotes&quot;) # install the remotes package pkgs = c( &quot;stats19&quot;, # downloads and formats open stats19 crash data &quot;sf&quot;, # spatial data package &quot;tidyverse&quot;, # a &#39;metapackage&#39; with many functions for data processing &quot;tmap&quot;, # for making maps &quot;pct&quot;, # access travel data from DfT-funded PCT project &quot;stplanr&quot; # transport planning tools ) remotes::install_cran(pkgs) lapply(pkgs, library, character.only = TRUE) 1.8 Overview The rest of the book is structured as follows. Chapter 2 introduces the basics of the R language. While not essential reading for people who already have experience with R or who just want to get stuck-in reading in datasets, as per Chapter 5, it is recommended reading even if you already use R. The chapter will introduce key aspects of the R language that may not be needed for basic data analysis tasks but which will be vital when debugging your code. This chapter provides strong foundations for subsequent chapters. Chapter 3 provides a brief introduction to productive research workflows using RStudio, an advanced integrated development environment for not only writing R code but for project management and boosting your productivity with a suite of features that puts Excel to shame. Chapter 4 introduces the stats19 package and other R packages we will be using in subsequent chapters. Chapter 5 demonstrates key data processing techniques using the tidyverse. Chapter 6 teaches key functions for working with time stamps. Chapter 7 shows how you can create maps and do geographic data analysis with road crash data in R. Chapter 8 provides an introduction to joining road crash data, with a focus on casualty and accident tables in STATS19 data (introduced in Chapter 4). Chapter 9 suggests next steps for road safety researchers looking to take their skills to the next levels and provide the strong evidence needed to safe lives. See Peng, Dominici, and Zeger (2006) for a detailed account of how to ensure reproducibility.↩︎ Unlike with general purpose languages such as Python — which requires you to install add-on packages such as Pandas before undertaking common data analysis tasks — R allows you to process, visualise and even model datasets out of the box using the ‘base’ packages (see An Introduction to R for details).\nBase R consists of core packages such as base, stats and graphics which contain functions for data processing, modelling and visualisation, respectively.\nInstalling additional packages is covered in Section 1.7.↩︎ If you are interested in the relative strengths and weaknesses of R in comparison with other languages for data science, I recommend a YouTube video titled ‘R or Python: Which Should You Learn in 2020’ and other recent videos by an experienced data scientist who goes by the name of ‘RichardOnData’.↩︎ The two main reasons why people cannot install recent versions of R/RStudio are lack of access to hardware or lack of admin rights on your computer.\nR is supported by most organisations and if you are having issues installing an up-to-date version of R/RStudio, we recommend politely asking your system administrator for support.↩︎ "],
["basics.html", "2 R basics 2.1 Creating and removing R objects 2.2 Classes and object length 2.3 Subsetting by index or name 2.4 Subsetting by values 2.5 Dealing with NAs and recoding 2.6 Changing class 2.7 Recoding values 2.8 Saving R objects 2.9 Now you are ready to use RStudio", " 2 R basics Learning a programming language is similar to learning a human language such as French. Although you could just dive in and start gesticulating to people in central Paris, it’s worth taking time to understand of the structure and key words of the language first. The same applies to data science: it will help to understand a little about syntax and grammar of the R language before diving into using it for applications such as road safety research. This chapter may seem tedious for people who just want to crack on and load-in data. However, working through the examples below is recommended for most people unless you’re already an experienced R user, and even experienced R users may learn something about the language’s unique syntax in the following sections. The first step is to start RStudio, e.g. by tapping the Start button and searching for RStudio if you are on Windows. You should see an ‘R console’ like that displayed in Figure 2.1. Figure 2.1: The R console in RStudio. You should see something like this when you first open RStudio. Tapping Enter at this point will trigger the autocompletion of the command `install.packages(“stats19”). If you saw something like that shown in Figure 2.1 congratulations! You are ready to start running R code, by entering commands into the console. 2.1 Creating and removing R objects R can be understood as a giant calculator. If you feed the console arithmetic tasks, it will solve them precisely and instantly. Try typing the following for examples (note that pi is an inbuilt object) into the R console in RStudio and pressing Enter to make R run the code: 2 + 19 ## [1] 21 pi^(19 + 2) / exp(2 + 19) ## [1] 20.89119 Use the same approach to find the square route of 361 (answer not shown): sqrt(361) This is all well and good, providing a chance to see how R works with numbers and get practice with typing commands into the console. However, a key benefit of R is that it is object oriented, meaning it stores values and complex objects such as data frames representing road casualties and processes them in memory (meaning that R is both fast and memory hungry when working with large datasets). The two most common ways of creating objects are with the the &lt;- ‘arrow’ or = ‘equals’ assignment operators.5 Let’s reproduce the calculations above but using objects to make the final command more concise: x = 2 y = 19 z = x + y pi^z / exp(z) ## [1] 20.89119 The previous code chunk created three objects called x, y and z and showed how these objects can themselves be used to create additional objects. Why x, y and z? Lack of imagination! You can call R objects anything you like, provided they do not start with numbers or contain reserved symbols such as + and -. You can use various stylistic conventions when naming your R objects, including camelCase and dot.case (???). We advocate using snake_case a style that avoids upper case characters to ease typing and uses the underscore symbol (_) to clearly demarcate spaces between words. These objects have now served their purpose. Based on wise saying that tidying up is the most important job, we will now remove these objects: rm(x, y, z) What just happened? We have removed the objects using the R function rm(), which stands for ‘remove’. Technically speaking, we passed the objects to arguments in the rm() function call. In plain English, things that go inside the curved brackets that follow a function name are arguments. The rm() function removes the objects that it is passed (most functions modify objects). A ‘nuclear option’ for cleaning your workspace is to run the following command, the meaning of which you will learn in the next section (can you guess?): rm(list = ls()) Next exercise: create objects that more closely approximate road casualty data by typing and running the following lines of code in the console: casualty_type = c(&quot;pedestrian&quot;, &quot;cyclist&quot;, &quot;cat&quot;) casualty_age = seq(from = 20, to = 60, by = 20) 2.2 Classes and object length We now have two objects with sensible names ‘in R’. Specifically, the objects are attached to the global environment, which means they can be listed with the ls() command: ls() ## [1] &quot;casualty_age&quot; &quot;casualty_type&quot; It should be clear from this that the above command rm(list = ls()) removes all objects in your global environment and should be used with caution. This also makes the wider point that functions can accept arguments (in this case the list argument of the rm() function) that are themselves function calls. Two key functions for getting the measure of R objects are class() and length(). class(casualty_type) ## [1] &quot;character&quot; class(casualty_age) ## [1] &quot;numeric&quot; The class of the casualty_type and casualty_type objects are character (meaning text) and numeric (meaning numbers), respectively. Next challenge: guess their length and check your guess was correct by running the following commands (results not shown): length(casualty_type) length(casualty_age) The next function we will use is data.frame() crashes = data.frame(casualty_type, casualty_age) Can you guess what that did? Find out by entering the following line of code, which will print the contents of the object (results not shown - you need to run the command on your own computer to see the result): crashes Without looking at its contents, we can get a handle of what is in the chrashes object as follows: class(crashes) ## [1] &quot;data.frame&quot; nrow(crashes) ## [1] 3 ncol(crashes) ## [1] 2 The results of the previous commands tell us that the dataset has 3 rows and 2 columns. We will use larger datasets, with thousands of rows and tens of columns, in later chapters but for now it’s good to ‘start small’ to understand the basics of R. 2.3 Subsetting by index or name Subsetting returns part of an R object. It can be done by providing numbers representing the positions of the elements we want (e.g. the 2nd element) or with a logical vector, with values associated with TRUE returned. Two dimension object such as matrices and data frames can be subset by rows and columns. Subsetting in base R is done with square brackets [] after the name of an object. Run the following commands to practice subsetting and verify that you get the same results that are shown below. casualty_age[2:3] # second and third casualty_age crashes[c(1, 2), ] # first and second row of crashes crashes[c(1, 2), 1] # first and second row of crashes, first column crashes$casualty_type # returns just one column The final command used the dollar symbol ($) to subset a column. We can use the same symbol to create a new column as follows: vehicle_type = c(&quot;car&quot;, &quot;bus&quot;, &quot;tank&quot;) crashes$vehicle_type = vehicle_type ncol(crashes) ## [1] 3 Notice that the dataset now has three columns after we added one to the right of the previous one. Note also that this would involve copying and pasting cells in Excel, but in R it is instant and happens as fast as you can type the command. To confirm what we think has happened has actually happened, let’s print out the object again to see its contents: crashes ## casualty_type casualty_age vehicle_type ## 1 pedestrian 20 car ## 2 cyclist 40 bus ## 3 cat 60 tank In Chapter 5 we will use filter() and select() functions to subset rows and columns. Before we get there, it is worth practicing subsetting using the square brackets to consolidate your understanding of how base R works with vector objects such as vehicle_type and data frames such as crashes. If you can answer the following questions, congratualations, you are ready to move on. If not, it’s worth doing some extra reading and practice on the topic of subsetting in base R. Exercises Use the $ operator to print the vehicle_type column of crashes. Subset the crashes with the [,] syntax so that only the first and third columns of crashes are returned. Return the 2nd row and the 3rd column of the crashes dataset. Return the 2nd row and the columns 2:3 of the crashes dataset. Bonus: what is the class() of the objects created by each of the previous exercises? 2.4 Subsetting by values It is also possible to subset objects by the values of their elements. This works because the [ operator accepts logical vectors returned by queries such as ‘is it less than 3?’ (x &lt; 3 in R) and ‘was it light?’ (crashes$dark == FALSE), as demonstrated below: x[c(TRUE, FALSE, TRUE, FALSE, TRUE)] # 1st, 3rd, and 5th element in x x[x == 5] # only when x == 5 (notice the use of double equals) x[x &lt; 3] # less than 3 x[x &lt; 3] = 0 # assign specific elements casualty_age[casualty_age %% 6 == 0] # just the ages that are a multiple of 6 crashes[crashes$dark == FALSE, ] Subset the casualty_age object using the inequality (&lt;) so that only elements less than 50 are returned. Subset the crashes data frame so that only tanks are returned using the == operator. Bonus: assign the age of all tanks to 61. 2.5 Dealing with NAs and recoding R objects can have a value of NA. This is how R represents missing data. z = c(4, 5, NA, 7) NA values are common in real-world data but can cause trouble, for example sum(z) # result is NA Some functions can be told to ignore NA values. sum(z, na.rm = TRUE) # result is equal to 4 + 5 + 7 You can find NAs using the is.na() function, and then remove them is.na(z) z_nona = z[!is.na(z)] # note the use of the not operator ! sum(z) If you remove records with NAs be warned: the average of a value excluding NAs may not be representative. 2.6 Changing class Sometimes you may want to change the class of an object. This is called class coercion, and can be done with functions such as as.logical(), as.numeric() and as.matrix(). Coerce the vehicle_type column of crashes to the class character. Coerce the crashes object into a matrix. What happened to the values? Bonus: What is the difference between the output of summary() on character and factor variables? 2.7 Recoding values Often it is useful to ‘recode’ values. In the raw STATS19 files, for example, -1 means NA. There are many ways to recode values in R, the simplest and most mature of which is the use of factors, as shown below: z = c(1, 2, -1, 1, 3) l = c(NA, &quot;a&quot;, &quot;b&quot;, &quot;c&quot;) # labels in ascending order z_factor = factor(z, labels = l) z_charcter = as.character(z_factor) z_charcter ## [1] &quot;a&quot; &quot;b&quot; NA &quot;a&quot; &quot;c&quot; Recode z to Slight, Serious and Fatal for 1:3 respectively. Bonus: read the help file at ?dplyr::case_when and try to recode the values using this function. 2.8 Saving R objects You can also save individual R objects in the RDS format. saveRDS(crashes, &quot;crashes.Rds&quot;) We can also read back in our data. crashes2 = readRDS(&quot;crashes.Rds&quot;) identical(crashes, crashes2) ## [1] TRUE R also supports many other formats, including CSV files, which can be created and imported with the functions readr::read_csv() and readr::write_csv() (see also the readr package). readr::write_csv(crashes, &quot;crashes.csv&quot;) crashes3 = readr::read_csv(&quot;crashes.csv&quot;) identical(crashes3, crashes) Notice that crashes3 and crashes are not identical, what has changed? Hint: read the help page associated with ?readr::write_csv. 2.9 Now you are ready to use RStudio Bonus: reproduce the following plot # eyes = c(2.3, 4, 3.7, 4) # eyes = matrix(eyes, ncol = 2, byrow = T) # mouth = c(2, 2, 2.5, 1.3, 3, 1, 3.5, 1.3, 4, 2) # mouth = matrix(mouth, ncol = 2, byrow = T) # # pdf(&quot;figures/smile.pdf&quot;) # # png(&quot;figures/smile.png&quot;) # plot(eyes, type = &quot;p&quot;, main = &quot;RRR!&quot;, cex = 2, xlim = c(1, 5), ylim = c(0, 5)) # lines(mouth, type = &quot;l&quot;, col = &quot;red&quot;) # dev.off() knitr::include_graphics(&quot;figures/smile.png&quot;) We use equals assignment for speed of typing and to avoid ambiguity in commands such as x&lt;-1 vs x&lt; -1, although &lt;- has historically been (and still is) more commonly used in many fields.↩︎ "],
["rstudio.html", "3 Using RStudio 3.1 Projects and scripts 3.2 Writing and running code 3.3 Viewing Objects 3.4 Autocompletion 3.5 Getting help 3.6 Commenting Code 3.7 The global environment 3.8 Debugging Code 3.9 Productivity boosting features", " 3 Using RStudio The previous chapter taught the basic’s of the R language. We entered and ran commands directly in the console. In this chapter we will learn how to write R scripts in RStudio’s source editor. We will also take a step back and considers how R code fits into the wider context of scripts, projects, and getting help in RStudio. RStudio is an integrated development environment (IDE) for R that makes it easy to create and run scripts, explore R objects and functions, plot results, and get help. The first exercise is to open up RStudio and take a look around and identify the main components, shown in Figure 3.1. Explore each of the main components of RStudio. Try clicking on different buttons in RStudio’s GUI and try changing the Global Settings (in the Tools menu) and see RStudio’s short cuts by pressing Alt-Shift-K (or Option+Shift+K on Mac). Figure 3.1: The RStudio user interface showing the four main ‘panes’. 3.1 Projects and scripts Projects organise files and settings in RStudio into folders. Each project has its own folder and Rproj file. When using RStudio always ensure you are working in a named project to organise your work. Start a new project with: File &gt; New Project You can choose to create a new directory (folder) or associate a project with an existing directory. Make a new project called lrrsrr (short for ‘learning reproducible road safety research with R’) or a name of you choice and save it in a sensible place on your computer. Notice that stats1-course now appears in the top right of RStudio. Scripts are files where R code is stored and you edit them in the Source Editor panel (the top left panel in Figure 3.1). Keeping your code in sensibly named, well organised and reproducible scripts will make your life easier. We could continue typing all our code into the console, as we did in Chapter 2. However that approach is limited when working on anything more complicated than a few simple commands. Code that you want to keep and share should be saved script files, plain text files that have the .R extension. Make a new script by typing and running this command in the R console:6 file.edit(&quot;chapter3.R&quot;) That will open the Source Editor and place your cursor there. Try jumping between the Source Editor and the Console by pressing Ctl+1 and Ctl+2. Keeping scripts and other files associated with a project in a single folder per project (in an RStudio project) will help you find things you need and develop an efficient workflow. To check that your project is saved, next close RStudio. In the next section we will re-open it and continue to edit the chapter3.R file. 3.2 Writing and running code Re-open RStudio and ensure that you have an empty file open in the Source Editor. We will type some basic commands into this file: type the following lines of code into your new chapter3.R R script and execute the result line-by-line by pressing Ctrl+Enter (Command+Enter on Mac): x = 1:5 y = c(0, 1, 3, 9, 18) plot(x, y) When the code is sent to the console two objects are created, both of which are vectors of 5 elements (bonus: check their length using the length() function). The third line of the code chunk plots them. Save the script by pressing Ctrl+S. There are several ways to run code within a script and it is worth becoming familiar with each. Try running the code you saved in the previous section using each of these methods: Place the cursor in different places on each line of code and press Ctrl+Enter to run that line of code. Highlight a block of code or part of a line of code and press Ctrl+Enter to run the highlighted code. Press Ctrl+Shift+Enter to run all the code in a script. Press the Run button on the toolbar to run all the code in a script. Bonus: Use the function source() to run all the code in a script e.g. source(\"chapter3.R\") Practice jumping between the console and the source editor by pressing Ctl+1 and Ctl+2. 3.3 Viewing Objects To practice typing code into scripts, rather than into the console, we will re-create the objects we created in Chapter 2. Create a new script called objects.R and type the following commands, character-for-character including spaces in the right places (typing rather than copy-pasting will help develop good coding style and speed): vehicle_type = c(&quot;car&quot;, &quot;bus&quot;, &quot;tank&quot;) casualty_type = c(&quot;pedestrian&quot;, &quot;cyclist&quot;, &quot;cat&quot;) casualty_age = seq(from = 20, to = 60, by = 20) set.seed(1) dark = sample(x = c(TRUE, FALSE), size = 3, replace = TRUE) small_matrix = matrix(1:24, nrow = 12) crashes = data.frame(vehicle_type, casualty_type, casualty_age, dark) Run the code line-by-line by pressing Ctl+Enter multiple times, as described in the previous section. Try viewing the objects in the following ways: Type the name of the object into the console, e.g. crashes and small_matrix, and run that code. Scroll up to see the numbers that didn’t fit on the screen. Use the head() function to view just the first 6 rows e.g. head(small_matrix) Bonus: use the n argument in the previous function call to show only the first 2 rows of small_matrix Click on the crashes object in the environment tab to View it in a spreadsheet. Run the command View(vehicle_type). What just happened? We can also get an overview of an object using a range of functions, including: summary() class() typeof() dim() length() View a summary of the casualty_age variable by running the following line of code (you should see the same output as shown below): summary(casualty_age) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 20 30 40 40 50 60 Exercise use the functions listed above (class() to length()) to test out basic R functions and get key information about the object vehicle_type. What does the output tell us about the object? 3.4 Autocompletion RStudio can help you write code by autocompleting it. RStudio will look for similar objects and functions after typing the first three letters of a name. When there is more than one option you can select from the list using the mouse or arrow keys. Within a function, you can get a list of arguments by pressing Tab. Test out RStudio’s amazing auto-completion capabilities by typing in the beginning of object names and functions and pressing Tab to see what suggestions pop-up. Try pressing Up and Down after pressing Tab to select different options. Bonus: try autocompleting file names by typing \"\" (the closing quote mark should be added automatically) and pressing Tab when your cursor is between the quote marks. What happens when you type \"~/\" and press Tab with your cursor just after the tilde (~) symbol. What does this mean (hint it involves the word ‘home’ and you can search the web to help get a full answer)? 3.5 Getting help Every function in R has a help page. You can view the help using ? for example ?sum and ?plot. Many packages also contain vignettes, these are long form help documents containing examples and guides. vignette() will show a list of all the vignettes available, or you can show a specific vignette for example vignette(topic = \"sf1\", package = \"sf\"). Try getting help on the stats19 package by typing the following and pressing Tab when your cursor is just to the left of the closing bracket ) (autocompletion works for more than just R objects and files - try making RStudio autocomplete and run the command vignette(\"stats19-vehicles\") for example): vignette(stats19) You can can further search and explore R’s help files using the Help panel in the bottom right window in RStudio. 3.6 Commenting Code It is good practice to use comments in your code to explain what it does. You can comment code using # For example: # Create vector objects (a whole line comment) x = 1:5 # a seqence of consecutive integers (inline comment) y = c(0, 1, 3, 9, 18.1) You can comment/uncomment a whole block of text by selecting it and using Ctrl+Shift+C. Pro tip: You can add a comment section using Ctrl + Shift + R 3.7 The global environment The Environment tab shows all the objects in your environment, this includes datasets, parameters, and any functions you have created. By default, new objects appear in the Global Environment but you can see other environments with the drop-down menu. For example, each package has its own environment. Sometimes you wish to remove things from your environment, perhaps because you no longer need them or things are getting cluttered. You can remove an object with the rm() function e.g. rm(x) or rm(x, y) or you can clear your whole environment with the broom button on the Environment Tab. Remove the object x that was created in a previous section. What happens when you try to print the x by entering it into the console? Try running the following commands in order: save.image(); rm(list = ls()); load(\".RData\"). What happened? How big (how many bytes) is the .RData file in your project’s folder? Tidy up by removing the .Rdata file with file.remove(\".Rdata\"). 3.8 Debugging Code All the code shown so far is reproducible and, unless you introduced typos, is ‘bug free’: it runs without errors. Typos are common though and even experienced R users frequently see error messages as they undertake interactive data analysis. For that reason learning to fix typos in R code is an important skill. RStudio comes to the rescue here with helpful debugging features. To test them out, write some code that fails as shown in the code chunk and Figure 3.2, and answer the questions below by interacting with RStudio: x = 1:5 y = c(0, 1, 3, 9 18.1) # R code with a typo ## Error: &lt;text&gt;:2:18: unexpected numeric constant ## 1: x = 1:5 ## 2: y = c(0, 1, 3, 9 18.1 ## ^ Figure 3.2: Debugging code with RStudio: notice the wavy red line highlighting a typo. Try running the faulty code, how can the error message help debug the code? What is the problem with the code shown in the figure? Create other types of error in the code you have run (e.g. no symetrical brackets and other typos) Does RStudio pick up on the errors? And what happens when you try to run buggy code? Always address debugging prompts to ensure your code is reproducible 3.9 Productivity boosting features This final section describes functionality in RStudio that goes beyond the features described above. RStudio is an advanced and powerful IDE and is highly customisable in myriad ways, especially since the launch of the RStudio Addins add-on system in 2016. Rather than try to be comprehensive (an impossible task), this section provides a list of additional RStudio features, starting simple and ending advanced, that have been tried, tested and proven to work, with links to the relevant documentation rather than extended descriptions. Zoom levels and appearance settings: it is important for code and other text to be the right size. Too small and it’s hard to see, too big and you end up frequently scrolling up and down. The appropriate text size varies: if you’re doing a screen share big text is appropriate; if you’re writing copious amounts of text (as I was when writing this prose in RStudio) smaller text will be handy. On Windows and Linux you can zoom with the shortcuts Ctl+Shift++ and Ctl+- for zooming in and out respectively. See Tools &gt; Global Options menu (which can be launched with the shortcut Alt+T and then G) for more advanced Appearance settings. Global search (and replace): in addition to search and replace functionality for single files (accessed in the standard way, by pressing Ctl+F), RStudio has a powerful global search feature inbuilt. Launch this feature by pressing Ctl+Shift+F and you can search any file types (e.g. only files ending in .R) for any string within an entire project. This feature is very handy when working with large, multi-file projects. Shortcuts: there are many, many shortcuts built into RStudio. In fact, there is even a shortcut to show the list of shortcuts. Try pressing Alt+Shift+K to get the complete list. Nobody I know can remember, let alone use, all of these. However, over time I expect that you will learn to love some of them. My top 5 RStudio-specific (there are many generic shortcuts such as Ctl+A to select all text) shortcuts are:7 Ctl+Enter to send a line of code from the code editor (called the Source Editor in RStudio) to be executed or ‘run’ in the console. Amazingly, some other prominent IDEs such as Microsoft’s VSCode editor, lack this important feature by default. Ctl+1 and Ctl+2 to switch between the console (for writing test code and ‘run once’ commands) and the code editor (for writing code to keep). Alt+Up/Down and Alt+Shift+Up/Down to move and copy lines of code up and down, handy when you want to re-order your code or make small changes to a copy of a line of code. Ctl+Shift+M will create the pipe operator (%&gt;%, this pipe was created using the shortcut!), saving time when creating dplyr pipelines, as discussed in Chapter 5. Ctl+Shift+F10 when you want to restart R, leaving you with a ‘blank slate’ in which packages are not loaded and objects are removed from the global environment Git integration for collaboration: RStudio provides two mechanisms for sharing your code with others via sites such as GitHub and GitLab, with the ‘Git’ panel in the top right pane and via the Terminal panel described in the next section. Support for Python, C++ and other languages: a joke on Twitter said “What’s the best Python editor? RStudio.” Although most Python programmers would probably disagree, the joke is true in the sense that R has good support for some other languages, Python and C++ in particular. If you open a Python script in RStudio on a computer that has Python and the reticulate R package installed, the R console will magically convert into a Python console when you press Ctl+Enter to execute a line of Python code, as described in the article “Reticulated Python” on the RStudio website. Like R package, an active community of developers is developing a range of extensions and RStudio itself is gradually evolving to meet the evolving needs of 21st Century data scientists. If there are any features that you would like to see, you can always ask others for pointers, e.g. on the RStudio Community forum. You can also create new files by clicking on File &gt; New File &gt; R Script with the mouse, or with the keyboard shortcut by pressing Ctrl+Shift+N. You can save the script and give it a sensible name like chapter3.R with File &gt; Save or Ctrl+S.\nWe recommend using the command file.edit() to create files, however, because it is faster and develops your typing skills.↩︎ See online articles such as “23 RStudio Tips, Tricks and Shortcuts” for more comprehensive lists of useful RStudio shortcuts.↩︎ "],
["pkgs.html", "4 R packages 4.1 What are packages? 4.2 The stats19 R package 4.3 Installing packages 4.4 Loading packages 4.5 Using packages 4.6 Updating packages 4.7 ggplot2 4.8 dplyr", " 4 R packages 4.1 What are packages? R has over 15,000 packages published on the official ‘CRAN’ site and many more published on code sharing sites such as GitHub. Packages are effectively plugins for R, that extend it in many ways. Packages are useful because they enhance the range of things you can do with R, providing additional functions, data and documentation that build on the core (or technically ‘base’) R packages. They range from general-purpose packages such as tidyverse and sf to domain-specific packages such as stats19. This chapter demonstrates the package lifecycle with reference stats19 and provides a taster of R’s visualisation capabilities with reference to the general purpose packages ggplot2 and dplyr. The stats19 package is particularly relevant for reproducible road safety research: its purpose is to download and clean road traffic casualty data from the UK’s Department for Transport. Domain-specific packages such as stats19 are often written by subject-matter experts, providing tried and tested solutions in a particular specialism. Regardless of which packages you install and use, they all go through the following main stages:8 installing it loading it using it updating it Of these, the third stage takes by far the most amount of time. Stages 1, 2 and 4 are equally important, however: you cannot properly use a package unless it has been properly installed, loaded and, to get the best performance out of the latest version, updated when new versions are released. We will learn each of these stages of the package lifecycle with the stats19 package. 4.2 The stats19 R package Like many packages, stats19 was developed to meet a real world need. STATS19 data is provided as a free and open resource by the Department for Transport, encouraging evidence-based and accountable road safety research and policy interventions. However, researchers at the University of Leeds found that repeatedly downloading and formatting open STATS19 data was time-consuming, taking valuable resources away from more valuable (and fun) aspects of the research process. Importantly, manually recoding the data, from 1 to Fatal for example, was error-prone. By packaging-up code to reliably solve the problem for ourselves, we found that we could solve the problem in a free, open and reproducible way for everyone (Lovelace et al. 2019). By abstracting the process to its fundamental steps (download, read, format), the package makes it easy to get the data into appropriate formats (of classes tbl, data.frame and sf), ready for for further processing and analysis steps. The package built on previous work (Lovelace, Roberts, and Kellar 2016) with several important improvements, including the conversion of crash data into geographic data into sf data frame for geographic research (e.g. Austin, Tight, and Kirby 1997). A useful feature of the package is that it enables creation of geographic representations of the data, geo-referenced to the correct coordinate reference system, in a single function, called format_sf(). Part funded by the RAC Foundation, the package should be of use to academic researchers and professional road safety data analysts working at local authority and national levels in the UK. The following sections demonstrate how to install, load and use packages with reference to stats19, know-how that can be used to install and use any package. 4.3 Installing packages The stats19 package is available on CRAN. This means that it has a web page on the CRAN website at cran.r-project.org — see cran.r-project.org/package=stats19 — with useful information including who developed the package, what the latest version is and when it was last updated. More importantly, being ‘on CRAN’ (which technically means ‘available on the Comprehensive R Archive Network’) means that that it can be installed with the command install.packages() as follows:9 install.packages(&quot;stats19&quot;) You might think that now that the package has been loaded we can start using it, but that is not true, as illustrated below command, which tries and fails to run the find_file_name() function from the package (check that this function exists by running the following command ?find_file_name) to find the file containing STATS19 casualties data for the year 2019: find_file_name(years = 2019, type = &quot;casualties&quot;) ## Error in find_file_name(years = 2019, type = &quot;casualties&quot;): could not find function &quot;find_file_name&quot; 4.4 Loading packages After you have installed a package the next step is to ‘load’ it.10 Load the stats19 package, that was installed in the previous section, as follows: library(stats19) ## Data provided under OGL v3.0. Cite the source and link to: ## www.nationalarchives.gov.uk/doc/open-government-licence/version/3/ What happened? Other than the message telling us about the package’s datasets (most packages load silently so do not worry if nothing happens when you load a package), the command above made the functions and datasets in the package available to us. Now we can use functions from the package without an error message, as follows: find_file_name(years = 2019, type = &quot;casualties&quot;) ## [1] &quot;DfTRoadSafety_Casualties_2019.zip&quot; This raises the question: how do you know which functions are available in a particular package? You can find out using autocompletion, by pressing Tab after typing the package’s name followed by two colons. Try typing stats19:: and then hitting Tab, for example. You should see a load of function names appearing which you view by pressing Up and Down on your keyboard. The final thing to say about packages is that you can use them without loading them by typing package::function(). stats19::find_file_name(years = 2019, type = \"casualties\") works even if the package isn’t loaded. You can test this by running the following command (which reports the versions of key geographic libraries you have installed on your system) in three different ways, the first of which fails but the second of which succeed: # try running a function without loading the package first sf_extSoftVersion() ## Error in sf_extSoftVersion(): could not find function &quot;sf_extSoftVersion&quot; # run a function from a package&#39;s namespace without loading it using :: sf::sf_extSoftVersion() ## GEOS GDAL proj.4 GDAL_with_GEOS USE_PROJ_H ## &quot;3.8.0&quot; &quot;3.0.4&quot; &quot;6.3.1&quot; &quot;true&quot; &quot;true&quot; # fun a function call after loading the package (the most common way) library(sf) ## Linking to GEOS 3.8.0, GDAL 3.0.4, PROJ 6.3.1 sf_extSoftVersion() ## GEOS GDAL proj.4 GDAL_with_GEOS USE_PROJ_H ## &quot;3.8.0&quot; &quot;3.0.4&quot; &quot;6.3.1&quot; &quot;true&quot; &quot;true&quot; As a bonus, try running the command sf::sf_extSoftVersion without the brackets (). What does that tell you about the package? 4.5 Using packages After loading a package, as described in the previous section, you can start using its functions. In the stats19 package, that means the following command, which uses the stats19 function get_stats19(), will work: crashes_2019 = get_stats19(year = 2019, type = &quot;accidents&quot;) nrow(crashes_2019) ## [1] 117536 The command demonstrates the value of packages. It would have been possible to get the same dataset by manually downloading and cleaning the file from the STATS19 website on data.gov.uk. However, but using the package the process has happened much faster and with fewer lines of code than would have been possible using general-purpose base R functions. The result of the nrow() function call shows that we have downloaded a decent amount of data representing over 100k road traffic casualty incidents across Great Britain in 2019. We will use other functions from the package in subsequent chapters. If you would like learn more about stats19 and how it can be used for road safety research check out its vignettes. The stats19 vignette, for example, should appear in the Help panel in the bottom right panel in RStudio after running the following command: vignette(&quot;stats19&quot;) 4.6 Updating packages Update packages with the command update.package() or in Tools &gt; Check for Package Updates in RStudio. You only need to install a package once but packages can be updated many times. To update just one package, you can give the function a package name, e.g.: update.packages(oldPkgs = &quot;stats19&quot;) Completing the following short exercises will ensure you’ve got a good understanding of packages and package versions. Take a look in the Packages tab in the Files pane in RStudio (bottom right by default). What version of the stats19 package is installed on your computer? What happens the second time you run update.packages(). Why? 4.7 ggplot2 ggplot2 is a generic plotting package that is part of the ‘tidyverse’ meta-package, an “opinionated collection of R packages designed for data science”. ggplot2 is flexible, popular, and has dozens of add-on packages which build on it, such as gganimate. To plot non-spatial data, it works as follows (the command should generate the image shown in Figure 4.1): library(ggplot2) ggplot(crashes_2019) + geom_bar(aes(date), width = 1) Figure 4.1: A simple ggplot2 graph. A key feature of the ggplot2 package is the initiation function ggplot2(), which takes a data object as its main augument, followed by one or more ‘geoms’ that represent layers (in this case a bar chart represented by the function geom_bar()). Another distinctive feature of ggplot2() is the use of + operator to add layers. The package is excellent for generating publication quality figures. Starting from a basic idea, you can make incremental tweaks to a plot to get the output you want. Building on the figure above, we could make the bin width wider and add colour depending on the crash severity, for example, as follows: ggplot(crashes_2019) + geom_bar(aes(date, fill = accident_severity), width = 1) ggplot(crashes_2019) + geom_bar(aes(date, fill = accident_severity), width = 1, position = &quot;fill&quot;) + ylab(&quot;Proportion of crashes&quot;) Figure 4.2: Demonstation of fill and position arguments in ggplot2. The package is huge and powerful, with support for a very wide range of plot types and themes, so it is worth taking time to read the documentation associated with the package, starting with the online reference manual and heading towards the online version of the package’s official book (Wickham 2016). As a final taught bit of ggplot2 code in this chapter, create a facetted plot showing how the number of crashes per hour varies across the days of the week by typing the following into the Source Editor and running the chunk line-by-line (the meaning of the commands should become clear by the end of the next Chapter): library(tidyverse) crashes_2019 %&gt;% mutate(hour = lubridate::hour(datetime)) %&gt;% mutate(day = lubridate::wday(date)) %&gt;% filter(!is.na(hour)) %&gt;% ggplot(aes(hour, fill = accident_severity)) + geom_bar(width = 1.01) + facet_wrap(~day) Figure 4.3: A plot showing a facetted time series plot made with ggplot2. Install a package that build on ggplot2 that begins with with gg. Hint: enter install.packages(gg) and hit Tab when your cursor is between the g and the ). Open a help page in the newly installed package with the ?package_name::function() syntax. Attach the package. Bonus: try using functionality from the new ‘gg’ package building on the example above to create plots like those shown below (hint: the right plot below uses the economist theme from the ggthemes package, try other themes). 4.8 dplyr Another useful package in the tidyverse is dplyr, which stands for ‘data pliers’ and provides a handy syntax for data manipulation. dplyr has many functions for manipulating data frames and using the pipe operator %&gt;%. The pipe puts the output of one command into the first argument of the next, as shown below (note the results are the same): library(dplyr) class(crashes) ## [1] &quot;data.frame&quot; crashes %&gt;% class() ## [1] &quot;data.frame&quot; We will learn more about this package and its other functions in Chapter 5. If for whatever reason you want to uninstall a package you can uninstall it in a fifth stage with commands such as remove.packages(\"stats19\").↩︎ To install the development version, which may have new features or but fixes that are not yet on CRAN, you can use the functionremotes::install_github(\"org/pkg\").\nThe stats19 package is hosted on the rOpenSci organisation at github.com/ropensci/stats19, so you can install the development version with remotes::install_github(\"ropensci/stats19\") (you must have the remotes package installed before that will work).\nNote: it’s usually safest to stick with the latest version on CRAN unless you know what you’re doing.↩︎ Technically, this means that the package namespace has been attached to the search path, making their functions available from the global environment (Wickham 2014).↩︎ "],
["data.html", "5 Manipulating data 5.1 tibbles 5.2 filter() and select() rows and columns 5.3 Ordering and selecting the ‘top n’ 5.4 Summarise 5.5 Tidyverse exersises", " 5 Manipulating data This chapter provides an introduction to manipulating datasets using the dplyr package. As outlined in the previous chapter, dplyr and ggplot2 are part of the tidyverse, which aims to provide a user friendly framework for data science (Grolemund and Wickham 2016). Experience of teaching R over the past few years suggests that many people find it easier to get going with data driven research if they learn the ‘tidy’ workflow presented in this chapter. The aim is not to impose this style and if you do not like this style of R code, or if you are simply curious, we encourage you to try alternative approaches for achieving the similar results using base R (R Core Team 2020b)11 the data.table R package (Dowle and Srinivasan 2019) or other languages such as Python or Julia. If you just want to get going with processing data, the tidyverse is a solid and popular starting point. Before diving into the tidyverse, it’s worth re-capping where we have got to so far, as we’ve covered a lot of ground. Chapter 2 introduced R’s basic syntax; Chapter 3 showed how to use the Source Editor and other features of RStudio to support data science; and Chapter 4 introduced the concept and practicalities of R packages, with reference to stats19, ggplot2 and dplyr. In this chapter, we will start with a blank slate. In Chapter 2 we learned that in R having a ‘clear desk’ means an empty global environment. This can be achieved by running the following command, that removes the list() of objects returned by ls(): rm(list = ls()) 5.1 tibbles Although the data processing techniques are capable of handling large datasets — such as the crashes_2019 object representing 100k+ casualties that we created in the previous Chapter (and which we will revisit in the next chapter) — it makes sense to start small. The starting point is to re-create the crashes dataset from Chapter 2, but using the tibble() function, the tidyverse equivalent of base R’s data.frame. tibble objects can be created, after loading the tidyverse, as follows: library(tidyverse) crashes = tibble( casualty_type = c(&quot;pedestrian&quot;, &quot;cyclist&quot;, &quot;cat&quot;), casualty_age = seq(from = 20, to = 60, by = 20), vehicle_type = c(&quot;car&quot;, &quot;bus&quot;, &quot;tank&quot;), dark = c(TRUE, FALSE, TRUE) ) In the previous code chunk we passed four vector objects as named arguments to the tibble function, resulting in columns such as casualty_type A tibble is basically just a fancy way of representing data.frame objects preferred by tidyverse users. It has a few sensible defaults compared with the data.frame, one of which can be seen by printing a tibble: class(crashes) ## [1] &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; crashes ## # A tibble: 3 x 4 ## casualty_type casualty_age vehicle_type dark ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;lgl&gt; ## 1 pedestrian 20 car TRUE ## 2 cyclist 40 bus FALSE ## 3 cat 60 tank TRUE Note the &lt;chr&gt; and &lt;dbl&gt; text below each column, providing a quick indication of the class of each variable. 5.2 filter() and select() rows and columns In the previous Chapter we briefly introduced the package dplyr, which provides an alternative to base R for manipulating objects. Basic operations for subsetting rows (with the command filter()) and columns (with the command select()) are demonstrated below. crashes %&gt;% filter(casualty_age &gt; 50) # filter rows ## # A tibble: 1 x 4 ## casualty_type casualty_age vehicle_type dark ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;lgl&gt; ## 1 cat 60 tank TRUE crashes %&gt;% select(casualty_type) # select just one column ## # A tibble: 3 x 1 ## casualty_type ## &lt;chr&gt; ## 1 pedestrian ## 2 cyclist ## 3 cat It should be clear what happened: filter() returns only rows that match the criteria in the function call, only observations with a casualty_age greater than 50 in this case. Likewise, select() returns data objects that include only columns named inside the function call, casualty_type in this case. To gain a greater understanding of the functions type and run the following commands: crashes_darkness = crashes %&gt;% filter(dark) crashes_a = crashes %&gt;% select(contains(&quot;a&quot;)) crashes_darkness_a = crashes %&gt;% filter(dark) %&gt;% select(contains(&quot;a&quot;)) Can you guess what the dimensions of the resulting objects will be? Write down your guesses for the number of rows and number of columns that the new objects, crashes_darkness to crashes_darkness_a, have before running the following commands to find out (which also demonstrates the handy function dim(), short for dimension, results not shown):12 dim(crashes) dim(crashes_darkness) ?contains # get help on contains() to help guess the output of the next line dim(crashes_a) dim(crashes_darkness_a) Look at the help pages associated with filter(), select() and the related function slice() as follows and try running the examples that you will find at the bottom of the help pages for each to gain a greater understanding (note you can use the package::function notation to get help on functions also): ?dplyr::filter ?dplyr::select ?dplyr::slice 5.3 Ordering and selecting the ‘top n’ Other useful pipe-friendly functions are arrange() and top_n(). We can use these functions to arrange datasets and take the top most ‘x’ values, as follows: crashes %&gt;% arrange(vehicle_type) ## # A tibble: 3 x 4 ## casualty_type casualty_age vehicle_type dark ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;lgl&gt; ## 1 cyclist 40 bus FALSE ## 2 pedestrian 20 car TRUE ## 3 cat 60 tank TRUE crashes %&gt;% top_n(n = 1, wt = casualty_age) ## # A tibble: 1 x 4 ## casualty_type casualty_age vehicle_type dark ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;lgl&gt; ## 1 cat 60 tank TRUE 5.4 Summarise A powerful two-function combination is group_by() and summarise(). Used together, they can provide grouped summaries of datasets. In the example below we find the mean age of casualties in dark and light conditions. crashes %&gt;% group_by(dark) %&gt;% summarise(mean_age = mean(casualty_age)) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 2 x 2 ## dark mean_age ## &lt;lgl&gt; &lt;dbl&gt; ## 1 FALSE 40 ## 2 TRUE 40 The example above shows a powerful feature of these pipelines: many operations can be ‘chained’ together, whilst keeping readability with subsequent commands stacked below earlier operations. Another useful feature of the tidyverse from a user perspective is auto-completion of column names mid pip. If you have not noticed this already, you can test it by typing the following, putting your cursor just before the ) and pressing Tab: crashes %&gt;% select(ca) # press Tab when your cursor is just after the a You should see casualty_age and casualty_type pop up as options that can be selected by pressing Up and Down. This may not seem like much, but when analysing large datasets with dozens of variables, it can be a godsend. The chapter has not provided a comprehensive introduction to the tidyverse fleet of packages but should provide enough to get started with using it for road safety data analysis. For further information, check out up-to-date online courses from respected organisations like Data Carpentry and the free online books such as R for Data Science (Grolemund and Wickham 2016). 5.5 Tidyverse exersises Use dplyr to filter row in which casualty_age is less than 18, and then 28. Use the arrange function to sort the crashes object in descending order of age (hint: see the ?arrange help page). Read the help page of dplyr::mutate(). What does the function do? Use the mutate function to create a new variable, birth_year, in the crashes data.frame which is defined as the current year minus their age. Bonus: Use the %&gt;% operator to filter the output from the previous exercise so that only observations with birth_year after 1969 are returned. Run the command help.start() to see a resources introducing base R, and Chapter 6 on lists and data frames in An Introduction to R in particular for an introduction to data manipulation with base R.↩︎ Note that the number of rows is reported before the number of columns.\nThis is a feature of R: rows are alse specificiefied first when subsetting using the square brackets in commands such as crashes[1, 2:3].↩︎ "],
["time.html", "6 Temporal data 6.1 Temporal analysis of crash data 6.2 Handling date and dat 6.3 Hours, minutes seconds with hms 6.4 The lubridate package 6.5 Dates in a data frame 6.6 Components of time objects", " 6 Temporal data Time is ubiquitous in road safety data: collisions and road safety implementations always happened at some point in time. Before demonstrating how to handle time series data in base R and with hms and lubridate packages, in subsequent sections, we will show how you can analyse the temporal dimensions of the real world crashes_2019 object we created in Chapter 4. The aim is to get you up-to-speed with how data analysis with time data ‘feels’ before learning the details in subsequent sections. If you’re the kind of person who likes to know the details first, feel free to skip the next section and return to it after having read the other sections. 6.1 Temporal analysis of crash data To get a feel for temporal data analysis in R, let’s start by reading-in crash data for 2019 with the stats19 package by typing the following into the Source Editor and running the code line-by-line, as taught in Chapter 3: library(stats19) crashes_2019 = get_stats19(2019) Note that, unlike the longer crashes_2019 = get_stats19(year = 2019, type = \"accidents\") used in Chapter 4, we did not use named arguments in this code chunk. Instead of year = 2019, we simply typed 2019. That is possible because R functions can be specified by name matching or order: the first argument of get_stats() is year so the function is expecting a year value. Also, although we didn’t explicitly specify the accidents table, type = \"accidents\" is the default value, so type only needs to be specified when importing casualty and vehicle datasets. With that educational aside out of the way, let’s take a look at the time variables that are actually in out newly read-in dataset: library(tidyverse) crashes_2019 %&gt;% select(matches(&quot;time|date&quot;)) %&gt;% names() ## [1] &quot;date&quot; &quot;time&quot; &quot;datetime&quot; Building on the previous chapter and a bit of guesswork, it should be clear what just happened: we selected variables that match (with the matches() function) the character strings \"time\" or (as indicated by the | vertical pipe symbol) \"date\" and returned the matching variable names. This shows that the stats19 package gives you not one, not two, but three temporal variables. Exercises: Print the first 6 and then the first 10 elements of each of the three temporal variables in crashes_2019 What is the class of each variable (technically, of each vector)? Bonus: Extract the weekday from the variable called date. Bonus: How many crashes happened on Monday? library(stats19) crashes_2017 = stats19::get_stats19(year = 2017, type = &quot;ac&quot;) crashes_2017 Of the three time variables, it should be clear from the outcome of previous exercises that datetime is contains the most useful information. To consolidate the plotting know-how learned in Chapter 4, let’s start by simply plotting the datetime object, a good way to understand new datasets and the variables they contain. Create the following three plots to show how date and time vary as a function of datetime: library(ggplot2) ggplot(crashes_2019) + geom_point(aes(datetime, date)) ggplot(crashes_2019) + geom_point(aes(datetime, time)) b = c(&quot;07:00&quot;, &quot;09:00&quot;, &quot;12:00&quot;, &quot;17:00&quot;, &quot;19:00&quot;) ggplot(crashes_2019) + geom_point(aes(datetime, time), alpha = 0.01) + scale_y_discrete(breaks = b) Figure 6.1: Three plots of the datetime (x axis) in relation to the date and time axis. Figure 6.1 tells us many things about the contents of the three temporal variables, and even provides insight into the temporal distribution of road casualties in Great Britain. The first two plots show 1) that the date variable is identical to the datetime variable (at least on the daily resolution than can be seen on the graph) and 2) that time values repeat regularly for the range of dates in datetime (from the start of Jan 2019 to end of Dec 2019). The third plot, which makes use of ggplot2‘s functionality to show only certain labels on the Y axis and reduced oppacity so that overlapping points are not completely black, is by far the most useful. It shows that most crashes happen between around 7am and 7pm, with a ’long tale’ of crashes in the evening, and that for most of the year there is a clear weekly cycle, reflecting the uptick in crashes during the rush hour commute on weekdays, a pattern that is greatly diminished during several weeks in summer (perhaps corresponding with summer holidays). The 52 weeks of the year can be distinguished even in this small and simple plot, highlighting the ability of visualisation to help understand data. Next, let’s look at how the time-of-day that crashes happen varies as a function of season, severity and day of week. From a datetime object of class POSIXct can be extracted any type of time information, including the minute, hour, day of week, and month of the crash (or other) event that the object records. Building on the time series plot we created in Section 4.7, let’s create a graph showing how the hourly distribution of crash numbers changes during the course of a working week. We will do this first by preprocessing the data, creating a new object called crashes_dow containing hour and day columns, and then plotting the result, after filtering out the weekend days, as shown in the code chunk below which results in Figure ??: # days of the week: dow = c(&quot;Sunday&quot;, &quot;Monday&quot;, &quot;Tuesday&quot;, &quot;Wednesday&quot;, &quot;Thursday&quot;, &quot;Friday&quot;, &quot;Saturday&quot;) crashes_dow = crashes_2019 %&gt;% mutate(hour = lubridate::hour(datetime)) %&gt;% mutate(day = factor(weekdays(date), levels = dow)) crashes_dow %&gt;% filter(!is.na(hour) &amp; !day %in% c(&quot;Saturday&quot;, &quot;Sunday&quot;)) %&gt;% ggplot(aes(hour)) + geom_bar(width = 1.01) + facet_wrap(~day, nrow = 1) Figure 6.2: Facetted time series showing how the number of crashes increases during the working week. The result is useful, but if we’re interested in the number of crashes per hour on different days of the week relative to the average, we need to undertake one more preprocessing step. We will count the number of crashes per hour for all 5 working days, divided by 5 to get the average, then count the number of crashes per hour/week combination and then divide the latter by the former. These steps are shown in the code chunk below, which results in Figure 6.3. crashes_dow_rel = crashes_dow %&gt;% filter(!is.na(hour) &amp; !day %in% c(&quot;Saturday&quot;, &quot;Sunday&quot;)) %&gt;% select(day, hour) %&gt;% group_by(hour) %&gt;% mutate(n_per_hour = n()/5) %&gt;% group_by(day, hour) %&gt;% summarise(n_hday = n(), n_h = first(n_per_hour)) %&gt;% mutate(hday_relative = n_hday / n_h) summary(crashes_dow_rel) ## day hour n_hday n_h ## Sunday : 0 Min. : 0.00 Min. : 68.0 Min. : 76.6 ## Monday :24 1st Qu.: 5.75 1st Qu.: 226.8 1st Qu.: 290.0 ## Tuesday :24 Median :11.50 Median : 790.0 Median : 794.1 ## Wednesday:24 Mean :11.50 Mean : 741.6 Mean : 741.6 ## Thursday :24 3rd Qu.:17.25 3rd Qu.:1006.2 3rd Qu.: 976.5 ## Friday :24 Max. :23.00 Max. :1756.0 Max. :1669.0 ## Saturday : 0 ## hday_relative ## Min. :0.7182 ## 1st Qu.:0.9397 ## Median :0.9925 ## Mean :1.0000 ## 3rd Qu.:1.0516 ## Max. :1.6198 ## crashes_dow_rel %&gt;% ggplot() + geom_col(aes(hour, hday_relative)) + facet_wrap(~day, nrow = 1) Figure 6.3: Facetted time series showing relative number of crashes per how by day in the working week. The results clearly show that Friday is a dangerous day. The extent to which the high relative number of crashes in the most anomalous hours, on Friday evening, is due increased exposure vs increased risk per km travelled cannot be ascertained by this plot but it certainly suggests that Friday afternoon and evening is a worthy focus of road safety research. Exercises: Building on the code above, show the absolute and relative number of crashes per hour on Saturday and Sunday. Filter the dataset so it contains only data from two police forces of your choice (e.g. West Yorkshire and Metropolitan Police). Try creating plots similar to those shown above but facetted by police force rather than by day of the week. 6.2 Handling date and dat it is worth remembering that base R already has decent support for dates and datetimes, although the base R functions are not particularly intuitive. This is shown in the code chunk below, which creates objects representing the date and time of a fictitious crash event on a cold winter’s morning, 1st January 2020, and a subsequent road safety intervention on the 20th October 2020: crash_datetime_character = &quot;2020-01-01 08:35&quot; crash_datetime = as.POSIXct(crash_datetime_character) class(crash_datetime) ## [1] &quot;POSIXct&quot; &quot;POSIXt&quot; intervention_date_character = &quot;2020-10-20&quot; intervention_date = as.Date(intervention_date_character) class(intervention_date) ## [1] &quot;Date&quot; # see ?as.POSIXct for more examples Fortunately for most users, there are easier ways to work with time series data, starting with the hms package. 6.3 Hours, minutes seconds with hms The hms library can be used to process hours, minutes and seconds, as shown below. See a very basic demo of the package and links to the package’s help pages with the following commands (in which we use the package without loading it with the library() function, demonstrating the package::function() syntax taught in Chapter 4: library(tidyverse) crash_time_character = &quot;08:35:00&quot; crash_time_hms = hms::as_hms(crash_time_character) class(crash_time_hms) ## [1] &quot;hms&quot; &quot;difftime&quot; ?hms::`hms-package` As the package’s name suggests it is used for dealing with hours minutes and seconds. It can round time objects of class hms to the nearest second (or any multiple of a second): hms::round_hms(crash_time_hms, 1) # time to the nearest second ## 08:35:00 hms::round_hms(crash_time_hms, 1 * 60 * 60) # time to the nearest hour ## 09:00:00 hms::round_hms(crash_time_hms, 1 * 30 * 60) # time to the nearest half hour ## 08:30:00 It can also convert simple text strings into time objects, e.g. as follows (note we do not need to include the :00): hms::parse_hm(&quot;08:35&quot;) ## 08:35:00 6.4 The lubridate package In many cases the most useful package easy of use when working with temporal data is lubridate. Load it as follows: library(lubridate) The simplest example of a Date object that we can analyze is just the current date, i.e. today() ## [1] &quot;2020-10-07&quot; We can manipulate this object using several lubridate functions to extract the current day, month, year, weekday and so on… x = today() day(x) wday(x) # Base R function to get the day of week weekdays(x) wday(x) %in% c(1, 6) # is it the weekend? month(x) year(x) Exercises: Look at the help page of the function month to see how it is possible to extract the current month as character vector Look at other functions in lubridate to extract the current weekday as a number, the week of year and the day of the year Date variables are often stored simply as a character vectors. This is a problem, since R is not always smart enough to distinguish between character vectors representing Dates. lubridate provides functions that can translate a wide range of date encodings such as ymd(), which extracts the Year Month and Day from a character string, as demonstrated below. as.Date(&quot;2019-10-17&quot;) # works as.Date(&quot;2019 10 17&quot;) # fails ymd(&quot;2019 10 17&quot;) # works dmy(&quot;17/10/2019&quot;) # works Import function such as read_csv try to recognize the Date variables. Sometimes this fails. You can manually create Date objects, as shown below. x = c(&quot;2009-01-01&quot;, &quot;2009-02-02&quot;, &quot;2009-03-03&quot;) x_date = ymd(x) x_date ## [1] &quot;2009-01-01&quot; &quot;2009-02-02&quot; &quot;2009-03-03&quot; Exercises: Extract the day, the year-day, the month and the weekday (as a non-abbreviated character vector) of each element of x_date. Convert \"09/09/93\" into a date object and extract its weekday. Bonus: Read the help page of as.Date and strptime for further details on base R functions for dates. Bonus: Read the Chapter 16 of R for Data Science book for further details on lubridate package. 6.5 Dates in a data frame We can use Dates also for subsetting events in a dataframe. For example, if we define x_date as before and add it to the crash dataset, i.e. crashes$casualty_day = x_date then we can subset events using Dates. For example filter(crashes, day(casualty_day) &lt; 7) # the events that ocurred in the first week of the month ## # A tibble: 3 x 5 ## casualty_type casualty_age vehicle_type dark casualty_day ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;lgl&gt; &lt;date&gt; ## 1 pedestrian 20 car TRUE 2009-01-01 ## 2 cyclist 40 bus FALSE 2009-02-02 ## 3 cat 60 tank TRUE 2009-03-03 filter(crashes, weekdays(casualty_day) == &quot;Monday&quot;) # the events occurred on monday ## # A tibble: 1 x 5 ## casualty_type casualty_age vehicle_type dark casualty_day ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;lgl&gt; &lt;date&gt; ## 1 cyclist 40 bus FALSE 2009-02-02 Exercises: Select only the events (rows in crashes) that occurred in January Select only the events that ocurred in an odd year-day Select only the events that ocurred in a leap-year (HINT: check the function leap_year) Select only the events that ocurred during the weekend or in June Select only the events that ocurred during the weekend and in June Count how many events ocurred during each day of the week. 6.6 Components of time objects Now we’ll take a look at the time components of a Date. Using the function hms (acronym for Hour Minutes Seconds) and its subfunctions such as hm or ms, we can parse a character vector representing several times as an Hour object (which is tecnically called a Period object). x = c(&quot;18:23:35&quot;, &quot;00:00:01&quot;, &quot;12:34:56&quot;) x_hour = hms(x) x_hour ## [1] &quot;18H 23M 35S&quot; &quot;1S&quot; &quot;12H 34M 56S&quot; We can manipulate these objects using several lubridate functions to extract the hour component, the minutes and so on: hour(x_hour) ## [1] 18 0 12 minute(x_hour) ## [1] 23 0 34 second(x_hour) ## [1] 35 1 56 If the Hour data do not specify the seconds, then we just have to use a subfunction of hms, namely hm, and everything works as before. x = c(&quot;18:23&quot;, &quot;00:00&quot;, &quot;12:34&quot;) (x_hour = hm(x)) ## [1] &quot;18H 23M 0S&quot; &quot;0S&quot; &quot;12H 34M 0S&quot; We can use Hour data also for subsetting events, like we did for Dates. Let’s add a new column to crashes data, crashes$casualty_hms = hms(c(&quot;18:23:35&quot;, &quot;00:00:01&quot;, &quot;12:34:56&quot;)) crashes$casualty_hour = hour(crashes$casualty_hms) Exercises: Filter only the events that ocurred after midday (i.e. the PM events). Hint: your answer may include &gt;= 12. Filter only the events that ocurred between 15:00 and 19:00 Bonus (difficult): run the following code, which downloades data for car crashes occurred during 2017. "],
["space.html", "7 Spatial data 7.1 sf objects 7.2 Reading and writing spatial data 7.3 sf polygons 7.4 Spatial subsetting and sf plotting 7.5 Geographic joins 7.6 CRSs 7.7 Buffers 7.8 Attribute operations on sf objects 7.9 Matching roads to crashes 7.10 Mapping road crash data 7.11 Analysing point data 7.12 Analysing crash data on road networks Bonus exercises", " 7 Spatial data 7.1 sf objects All road crashes happen somewhere and, in the UK at least, all collisions recorded by the police are given geographic coordinates, something that can help prioritise interventions to save lives by intervening in and around ‘crash hotspots’. R has strong geographic data capabilities, with the sf package provides a generic class for spatial vector data: points, lines and polygons, are represented in sf objects as a special ‘geometry column’, typically called ‘geom’ or ‘geometry’, extending the data frame class we’ve already seen in crashes. Create an sf data frame called crashes_sf as follows: library(sf) # load the sf package for working with spatial data crashes_sf = crashes # create copy of crashes dataset crashes_sf$longitude = c(-1.3, -1.2, -1.1) crashes_sf$latitude = c(50.7, 50.7, 50.68) crashes_sf = st_as_sf(crashes_sf, coords = c(&quot;longitude&quot;, &quot;latitude&quot;), crs = 4326) # plot(crashes_sf[1:4]) # basic plot # mapview::mapview(crashes_sf) # for interactive map Plot only the geometry column of crashes_sf (hint: the solution may contain $geometry). If the result is like the figure below, congratulations, it worked!). Plot crashes_sf, only showing the age variable. Plot the 2nd and 3rd crashes, showing which happened in the dark. Bonus: How far are the points apart (hint: sf functions begin with st_)? Bonus: Near which settlement did the tank runover the cat? 7.2 Reading and writing spatial data You can read and write spatial data with read_sf() and write_sf(), as shown below (see ?read_sf). write_sf(zones, &quot;zones.geojson&quot;) # save geojson file write_sf(zones, &quot;zmapinfo&quot;, driver = &quot;MapInfo file&quot;) read_sf(&quot;zmapinfo&quot;) # read in mapinfo file See Chapter 6 of Geocomputation with R for further information. 7.3 sf polygons sf objects can also represent administrative zones. This is illustrated below with reference to zones, a spatial object representing the Isle of Wight, that we will download using the pct package (note: the [1:9] appended to the function selects only the first 9 columns). zones = pct::get_pct_zones(&quot;isle-of-wight&quot;)[1:9] What is the class of the zones object? What are its column names? Print its first 2 rows and columns 6:8 (the result is below). ## Simple feature collection with 2 features and 5 fields ## geometry type: MULTIPOLYGON ## dimension: XY ## bbox: xmin: -1.301131 ymin: 50.69052 xmax: -1.28837 ymax: 50.70547 ## geographic CRS: WGS 84 ## # A tibble: 2 x 6 ## geo_code all bicycle foot car_driver geometry ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;MULTIPOLYGON [°]&gt; ## 1 E01017326 698 23 285 286 (((-1.289993 50.69766, -1.290177 50.… ## 2 E01017327 720 25 225 374 (((-1.295712 50.69383, -1.29873 50.6… 7.4 Spatial subsetting and sf plotting Like index and value subsetting, spatial subsetting can be done with the [ notation. Subset the zones that contain features in crashes_sf as follows: zones_containing_crashes = zones[crashes_sf, ] To plot a new layer on top of an existing sf plot, use the add = TRUE argument. Remember to plot only the geometry column of objects to avoid multiple maps. Colours can be set with the col argument. Plot the geometry of the zones, with the zones containing crashes overlaid on top in red. Plot the zone containing the 2nd crash in blue. Bonus: plot all zones that intersect with a zone containing crashes, with the actual crash points plotted in black. 7.5 Geographic joins Geographic joins involve assigning values from one object to a new column in another, based on the geographic relationship between them. With sf objects it works as follows: zones_joined = st_join(zones[1], crashes_sf) Plot the casualty_age variable of the new zones_joined object (see the figure below to verify the result). How many zones are returned in the previous command? Select only the geo_code column from the zones and the dark column from crashes_sf and use the left = FALSE argument to return only zones in which crashes occured. Plot the result. See Chapter 4 of Geocomputation with R (Lovelace, Nowosad, and Muenchow 2019) for further information on geographic joins. 7.6 CRSs Get and set Coordinate Reference Systems (CRSs) with the command st_crs(). Transform CRSs with the command st_transform(), as demonstrated in the code chunk below, which converts the ‘lon/lat’ geographic CRS of crashes_sf into the projected CRS of the British National Grid: crashes_osgb = st_transform(crashes_sf, 27700) Try to subset the zones with the crashes_osgb. What does the error message say? Create zones_osgb by transforming the zones object. Bonus: use st_crs() to find out the units measurement of the British National Grid? For more information on CRSs see Chapter 6 of Geocompuation with R. 7.7 Buffers Buffers are polygons surrounding geometries of a (usually) fixed distance. Currently buffer operations in R only work on objects with projected CRSs. Find out and read the help page of sf’s buffer function. Create an object called crashes_1km_buffer representing the area within 1 km of the crashes. Bonus: try creating buffers on the geographic version of the crashes_sf object. What happens? 7.8 Attribute operations on sf objects Because sf objects are data.frames, we can do non-spatial operations on them. Try the following attribute operations on the zones data. # load example dataset if it doesn&#39;t already exist zones = pct::get_pct_zones(&quot;isle-of-wight&quot;) sel = zones$all &gt; 3000 # create a subsetting object zones_large = zones[sel, ] # subset areas with a popualtion over 100,000 zones_2 = zones[zones$geo_name == &quot;Isle of Wight 002&quot;,] # subset based on &#39;equality&#39; query zones_first_and_third_column = zones[c(1, 3)] zones_just_all = zones[&quot;all&quot;] Practice subsetting techniques you have learned on the sf data.frame object zones: Create an object called zones_small which contains only regions with less than 3000 people in the all column Create a selection object called sel_high_car which is TRUE for regions with above median numbers of people who travel by car and FALSE otherwise Create an object called zones_foot which contains only the foot attribute from zones Bonus: plot zones_foot using the function plot to show where walking is a popular mode of travel to work Bonus: bulding on your answers to previous questions, use filter() from the dplyr package to subset small regions where car use is high. Bonus: What is the population density of each region (hint: you may need to use the functions st_area(), as.numeric() and use the ‘all’ column)? Bonus: Which zone has the highest percentage of people who cycle? 7.9 Matching roads to crashes I think you forgot something here. For example we could introduce st_nearest_feature? Or counting using st_within and st_buffer. 7.10 Mapping road crash data So far we have used the plot() function to make maps. That’s fine for basic visualisation, but for publication-quality maps, we recommend using tmap (see Chapter 8 of Geocomputation with R for reasons and alternatives). Load the package as follows: library(tmap) tmap_mode(&quot;plot&quot;) ## tmap mode set to plotting Create the following plots using plot() and tm_shape() + tm_polygons() functions (note: the third figure relies on setting tmap_mode(\"view\"). Add an additional layer to the interactive map showing the location of crashes, using marker and dot symbols. Bonus: Change the default basemap (hint: you may need to search in the package documentation or online for the solution). ## tmap mode set to interactive viewing 7.11 Analysing point data Based on the saying “don’t run before you can walk”, we’ve learned the vital foundations of R before tackling a real dataset. Temporal and spatial attributes are key to road crash data, hence the emphasis on lubridate and sf. Visualisation is key to understanding and policy influence, which is where tmap comes in. With these solid foundations, plus knowledge of how to ask for help (read R’s internal help functions, ask colleagues, create new comments on online forums/GitHub, generally in that order of priority), you are ready to test the methods on some real data. Before doing so, take a read of the stats19 vignette, which can be launched as follows: vignette(package = &quot;stats19&quot;) # view all vignettes available on stats19 vignette(&quot;stats19&quot;) # view the introductory vignette This should now be sufficient to tackle the following exercises: Download and plot all crashes reported in Great Britain in 2018 (hint: see the stats19 vignette) Find the function in the stats19 package that converts a data.frame object into an sf data frame. Use this function to convert the road crashes into an sf object, called crashes_sf, for example. Filter crashes that happened in the Isle of Wight based on attribute data (hint: the relevant column contains the word local) Filter crashes happened in the Isle of Wight using geographic subsetting (hint: remember st_crs()?) Bonus: Which type of subsetting yielded more results and why? Bonus: how many crashes happened in each zone? Create a new column called month in the crash data using the function lubridate::month() and the date column. Create an object called a_zones_may representing all the crashes that happened in the Isle of Wight in the month of May Bonus: Calculate the average (mean) speed limit associated with each crash that happened in May across the zones of the Isle of Wight (the result is shown in the map) 7.12 Analysing crash data on road networks Road network data can be accessed from a range of sources, including OpenStreetMap (OSM) and Ordnance Survey. We will use some OSM data from the Ilse of Wight, which can be loaded as follows: u = &quot;https://github.com/ropensci/stats19/releases/download/1.1.0/roads_key.Rds&quot; roads_wgs = readRDS(url(u)) roads = roads_wgs %&gt;% st_transform(crs = 27700) You should already have road crashes for the Isle of Wight from the previous stage. If not, load crash data as follows: u = &quot;https://github.com/ropensci/stats19/releases/download/1.1.0/car_accidents_2017_iow.Rds&quot; crashes_iow = readRDS(url(u)) Plot the roads with the crashes overlaid. Create a buffer around the roads with a distance of 200 m. How many crashes fall outside the buffered roads? Bonus: Use the aggregate() function to identify how many crashes happened per segment and plot the result (hint: see ?aggregate.sf and take a read of Section 4.2.5 of Geocomputation with R) with tmap and plot the crashes that happened outside the road buffers on top. Bonus exercises Identify a region and zonal units of interest from http://geoportal.statistics.gov.uk/ or from the object police_boundaries in the stats19 package. Read them into R as an sf object Create a map showing the number of crashes in each zone Identify the average speed limit associated with crashes in each zone Identify an interesting question you can ask to the data and use exploratory data analysis to find answers Check another related project for further information on smoothing techniques of counts on a linear network. "],
["join.html", "8 Joining road crash tables", " 8 Joining road crash tables Content on joining road crash tables. So far we’ve been working primarily with ‘accident’ level data but there is much useful data in other tables. Let’s read in data from 2019 to take a look: library(stats19) ac = get_stats19(year = 2019, type = &quot;ac&quot;, output_format = &quot;sf&quot;) ca = get_stats19(year = 2019, type = &quot;ca&quot;) ve = get_stats19(year = 2019, type = &quot;ve&quot;) Each table represents the same phenomena: road casualties in Great Britain in 2019. Therefore you may expect they would have the same number of rows, but that is not the case: nrow(ac) ## [1] 117508 nrow(ca) ## [1] 153158 nrow(ve) ## [1] 216381 The reason for this is that there are, on average, more than one casualty per crash (e.g. when a car hits two people) and more than one vehicle, including bicycles, per crash13 We can find the average number of casualties and vehicles per crash as follows: nrow(ca) / nrow(ac) ## [1] 1.303384 nrow(ve) / nrow(ac) ## [1] 1.841415 The output of the commands above show that there are around 1.3 casualties and 1.8 vehicles involved in each crash record in the STATS19 dataset for 2019. Each table each contains a different number of columns, reporting the characteristics of each casualty and each driver/vehicle for the ca and ve datasets respectively. ncol(ac) ## [1] 32 ncol(ca) ## [1] 16 ncol(ve) ## [1] 23 The output of the previous code chunk shows that we have more variables in the ‘accidents’ table than the others but the other tables are data rich with 16 columns on the casualties and 23 on the vehicles. The three main tables we have just read-in can be joined by shared key variables. This is demonstrated in the code chunk below, which subsets all casualties that took place in London, and counts the number of casualties by severity for each crash: library(tidyr) library(dplyr) # table(ac$police_force) lnd_police = c(&quot;City of London&quot;, &quot;Metropolitan Police&quot;) ac_lnd = ac %&gt;% filter(police_force %in% lnd_police) ca_lnd = ca %&gt;% filter(accident_index %in% ac_lnd$accident_index) cas_types = ca_lnd %&gt;% select(accident_index, casualty_type) %&gt;% group_by(accident_index) %&gt;% summarise( Total = n(), walking = sum(casualty_type == &quot;Pedestrian&quot;), cycling = sum(casualty_type == &quot;Cyclist&quot;), passenger = sum(casualty_type == &quot;Car occupant&quot;) ) cj = left_join(ac_lnd, cas_types) What just happened? We found the subset of casualties that took place in London with reference to the accident_index variable. Then we used the dplyr function summarise(), to find the number of people who were in a car, cycling, and walking when they were injured. This new casualty dataset is joined onto the crashes_lnd dataset. The result is a spatial (sf) data frame of ac in London, with columns counting how many road users of different types were hurt. The joined data has additional variables: base::setdiff(names(cj), names(ac_lnd)) ## [1] &quot;Total&quot; &quot;walking&quot; &quot;cycling&quot; &quot;passenger&quot; As a simple spatial plot, we can map all the crashes that have happened in London in 2017, with the colour related to the total number of people hurt in each crash. Placing this plot next to a map of London provides context: plot( cj[cj$cycling &gt; 0, &quot;speed_limit&quot;, ], cex = cj$Total[cj$cycling &gt; 0] / 3, main = &quot;Speed limit (cycling)&quot; ) plot( cj[cj$passenger &gt; 0, &quot;speed_limit&quot;, ], cex = cj$Total[cj$passenger &gt; 0] / 3, main = &quot;Speed limit (passenger)&quot; ) The spatial distribution of crashes in London clearly relates to the region’s geography. Car crashes tend to happen on fast roads, including busy Motorway roads, displayed in yellow above. Cycling is as an urban activity, and the most bike crashes can be found in near Leeds city centre, which has a comparatively high level of cycling (compared with the low baseline of 3%). This can be seen by comparing the previous map with an overview of the area, from an academic paper on the social, spatial and temporal distribution of bike crashes (Lovelace, Roberts, and Kellar 2016): In addition to the Total number of people hurt/killed, cj contains a column for each type of casualty (cyclist, car occupant, etc.), and a number corresponding to the number of each type hurt in each crash. It also contains the geometry column from ac_sf. In other words, joins allow the casualties and vehicles tables to be geo-referenced. We can then explore the spatial distribution of different casualty types. The following figure, for example, shows the spatial distribution of pedestrians and car passengers hurt in car crashes across London in 2017: library(ggplot2) ac_types = cj %&gt;% filter(accident_severity != &quot;Slight&quot;) %&gt;% mutate(type = case_when( walking &gt; 0 ~ &quot;Walking&quot;, cycling &gt; 0 ~ &quot;Cycling&quot;, passenger &gt; 0 ~ &quot;Passenger&quot;, TRUE ~ &quot;Other&quot; )) ggplot(ac_types, aes(size = Total, colour = speed_limit)) + geom_sf(show.legend = &quot;point&quot;, alpha = 0.3) + facet_grid(vars(type), vars(accident_severity)) + scale_size( breaks = c(1:3, 12), labels = c(1:2, &quot;3+&quot;, 12) ) + scale_color_gradientn(colours = c(&quot;blue&quot;, &quot;yellow&quot;, &quot;red&quot;)) + theme(axis.text = element_blank(), axis.ticks = element_blank()) Figure 8.1: Spatial distribution of serious and fatal crashes in London, for cycling, walking, being a car passenger and other modes of travel. Colour is related to the speed limit where the crash happened (red is faster) and size is proportional to the total number of people hurt in each crash (legend not shown). ac_names = names(ac) ac_names[grepl(pattern = &quot;cas&quot;, x = ac_names)] ## [1] &quot;number_of_casualties&quot; STATS19 data contains information on when a single crashes without involvement of any other vehicles, but not when a lone cyclist crashes without any other vehicle involved.↩︎ "],
["next.html", "9 Next steps 9.1 Automated reporting with RMarkdown 9.2 Sharing code 9.3 Asking questions 9.4 Use cases", " 9 Next steps for reproducible road safety research 9.1 Automated reporting with RMarkdown 9.2 Sharing code 9.3 Asking questions Slack group RSGB Analyst Network. 9.4 Use cases Hannah Bougdah Will Other Do you have a use case of reprocible research? Please get in touch on the issue tracker. "],
["references.html", "10 References", " 10 References "]
]
